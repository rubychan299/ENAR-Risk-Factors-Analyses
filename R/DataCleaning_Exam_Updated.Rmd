---
title: "ws"
output: html_document
date: "2025-02-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TODO:
https://docs.google.com/document/d/1IYLTEWNzi34iagvPAwtEt4nIZdpzWqoQ5zt4LkySbhg/edit?tab=t.0 
1. merge 2021-2023 data with original exam data 
2. extract my variables 2013-2023 and demographics variables with SEQN
3. recode variables based on excel logic, document everything
4. save RData and uplaod to github

```{r}
# load packages here
library(haven)
library(dplyr)
library(rpms)
library(ggplot2)
library(pROC)
source('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/R/funcs.R')
```

# Load 1999-2020 exam data (nhanesA format)
```{r}
rm(list = ls())
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/Examinations.RData')
```

# Save as dataframe with 1999-2020 data
```{r}
exnames <- c("AUX", "BPX", "BMX", 'DXX', 'BPXO', 'OHXDENT')

ex_dat <- ex_dat[names(ex_dat) %in% exnames]

ex_df <- combine_surveys(ex_dat)

save(ex_df, file = "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/Examinations_1999_to_2020.RData")
```

# Load 2021-2023 data for exam and demographics
```{r}
rm(list = ls())

load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/Examinations_1999_to_2020.RData')
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/Demographics.RData')
demo_df <- demo_dat$DEMO

BAX_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/exam/BAX_L.xpt")
BMX_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/exam/BMX_L.xpt")
BPXO_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/exam/BPXO_L.xpt")
LUX_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/exam/LUX_L.xpt")
DEMO_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/demographics/DEMO_L.xpt")
```

# Merge to create 1999-2023 data
```{r}
# Add Year = 2021 to all smaller datasets
datasets <- list(BAX_L, BMX_L, BPXO_L, LUX_L)

datasets <- lapply(datasets, function(df) {
  df <- as.data.frame(df)  # Ensure it's a data frame
  df <- df %>% mutate(Year = '2021')  # Add Year
  return(df)
})

# Perform full joins across all datasets
full_joined_data <- Reduce(function(x, y) full_join(x, y, by = c("SEQN", "Year")), datasets)

# Bind ex_df with the fully joined dataset
ex_df_joined <- bind_rows(ex_df, full_joined_data)


# Merge Demo data
datasets <- list(DEMO_L)
datasets <- lapply(datasets, function(df) {
  df <- as.data.frame(df)  # Ensure it's a data frame
  df <- df %>% mutate(Year = '2021')  # Add Year
  return(df)
})
demo_df_joined <- bind_rows(demo_df, datasets)
duplicate_seqn <- demo_df_joined %>%
  group_by(SEQN, Year) %>%
  filter(n() > 1)  # Keep only duplicates

# Display duplicate SEQN count
cat("Number of duplicated SEQN values:", nrow(duplicate_seqn), "\n")


# Saving 
demo_1999_to_2023 <- demo_df_joined
exam_1999_to_2023 <- ex_df_joined
save(demo_1999_to_2023, file = "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/demo_1999_to_2023.RData")
save(exam_1999_to_2023, file = "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/exam_1999_to_2023.RData")
```

# Extract necessary variables from the 1999-2023 data
```{r}
rm(list = ls())
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/exam_1999_to_2023.RData')

exvars <- c(
  # ID, YEAR
  "SEQN", "Year",
  
  # BPX (Blood Pressure Exam)
  "BPQ150A", "BPQ150B", "BPQ150C", "BPQ150D", 
  "BPXPULS", "BPXPTY", "BPXML1", 
  "BPXSY1", "BPXDI1", "BPAEN1", "BPXSY2", "BPXDI2", "BPAEN2", 
  "BPXSY3", "BPXDI3", "BPAEN3", "BPXSY4", "BPXDI4", "BPAEN4",
  "BPXOSY1", "BPXODI1", 
  "BPXOSY2", "BPXODI2", 
  "BPXOSY3", "BPXODI3", 
  "BPXOPLS1", "BPXOPLS2", "BPXOPLS3",
  
  # BMX (Body Measurements Exam)
  "BMXBMI", "BMXWAIST", "BMXHIP",
  
  # DXX (Dual-energy X-ray Absorptiometry - DXA Scan)
  "DXXLSBMD", "DXDTRBMD", "DXDTRPF", "DXXLABMD", 
  "DXDLAPF", "DXXRABMD", "DXDRAPF",
  
  # AUX (Audiometry Exam)
  "AUAEXSTS", "AUQ020", "AUQ030", 
  "AUXU1K1R", "AUXU500R", "AUXU1K2R", "AUXU2KR", "AUXU3KR", 
  "AUXU4KR", "AUXU6KR", "AUXU8KR", "AUXU1K1L", "AUXU500L", 
  "AUXU1K2L", "AUXU2KL", "AUXU3KL", "AUXU4KL", "AUXU6KL", "AUXU8KL",
  "AUXR1K1R", "AUXR5CR", "AUXR1K2R", "AUXR2KR", "AUXR3KR", "AUXR4KR", 
  "AUXR6KR", "AUXR8KR", "AUXR1K1L", "AUXR5CL", "AUXR1K2L", "AUXR2KL", 
  "AUXR3KL", "AUXR4KL", "AUXR6KL", "AUXR8KL",
  
  # OHX (Oral Health Exam - Dental)
  "OHDEXSTS", "OHAREC", "OHAROCDT", "OHAROCGP", "OHAROCOH", 
  "OHAROCCI", "OHAROCDE",
  
  # BMDAVSAD (Bone Mineral Density - DXA Scan)
  "BMDAVSAD"
)

exam_subset <- exam_1999_to_2023 %>%
  filter(Year %in% c("2013", "2015", "2017", "P", "2021")) %>%
  select(any_of(exvars))  

missingness <- exam_subset %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "Missing_Percentage")
print(missingness)

```
# Data re-coding
```{r}
# Define BP variables for general years and 2021
bp_vars_general <- c("BPXSY1", "BPXDI1", "BPXSY2", "BPXDI2", "BPXSY3", "BPXDI3", "BPXSY4", "BPXDI4")
bp_vars_2021 <- c("BPXOSY1", "BPXODI1", "BPXOSY2", "BPXODI2", "BPXOSY3", "BPXODI3")
# Compute mean and standard deviation separately for 2021 and other years
bp_stats_general <- exam_subset %>%
  filter(Year != 2021) %>%
  summarise(across(all_of(bp_vars_general), list(mean = ~ mean(. , na.rm = TRUE),
                                                 sd = ~ sd(. , na.rm = TRUE)))) %>%
  pivot_longer(cols = everything(), names_to = c("Variable", ".value"), names_sep = "_") %>%
  mutate(Year = "Other Years")

bp_stats_2021 <- exam_subset %>%
  filter(Year == 2021) %>%
  summarise(across(all_of(bp_vars_2021), list(mean = ~ mean(. , na.rm = TRUE),
                                              sd = ~ sd(. , na.rm = TRUE)))) %>%
  pivot_longer(cols = everything(), names_to = c("Variable", ".value"), names_sep = "_") %>%
  mutate(Year = "2021")
# Combine both statistics
bp_stats_long <- bind_rows(bp_stats_general, bp_stats_2021)
# Display the results
print(bp_stats_long)


# Since not much fluctuation between BP1-BP3, we will take average of 1,2,3 to create new numerical variables called BPXSY and BPXDI
# Dual-Energy X-ray 
# select DXDTRBMD, DXXLABMD based on low correlation with other bmi variables and DXD 
# 2021 and P is measured by BPXOSY, BPXODI
exam_bp <- exam_subset %>%
  mutate(
    BPXSY = ifelse(Year %in% c('2021', 'P'), 
                   rowMeans(select(., BPXOSY1, BPXOSY2, BPXOSY3), na.rm = TRUE) + 1.5, 
                   rowMeans(select(., BPXSY1, BPXSY2, BPXSY3), na.rm = TRUE)),
    BPXDI = ifelse(Year %in% c('2021', 'P'), 
                   rowMeans(select(., BPXODI1, BPXODI2, BPXODI3), na.rm = TRUE) - 1.3, 
                   rowMeans(select(., BPXDI1, BPXDI2, BPXDI3), na.rm = TRUE))
  ) %>%
  select(SEQN, Year, BPXSY, BPXDI, BPXPULS, BMXBMI, DXDTRBMD, DXXLABMD)  # Keep only relevant columns

# removing BMXWAIST since high correlation with BMXBMI
print(cor.test(exam_subset$BMXBMI, exam_subset$BMXWAIST))

missingness <- exam_bp %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "Missing_Percentage")
print(missingness)

# Encode into hearing loss vs noraml based on literature's threshold for normal hearing 
exam_audio <- exam_subset %>%
  rowwise() %>%  
  mutate(hearing_loss = ifelse(
    any(c(AUXU1K1R, AUXU500R, AUXU1K2R, AUXU2KR, AUXU3KR, 
          AUXU4KR, AUXU6KR, AUXU8KR, AUXU1K1L, AUXU500L, 
          AUXU1K2L, AUXU2KL, AUXU3KL, AUXU4KL, AUXU6KL, AUXU8KL) > 21, 
        na.rm = TRUE), 
    "No",   # Normal
    "Yes"   # Hearing Loss
  )) %>%
  select(SEQN, Year, hearing_loss)
table(exam_audio$hearing_loss)


exam_2013_to_2023 <- exam_bp %>%
  left_join(exam_audio, by = c("SEQN", "Year"))

# Define BP control defined as SBP <140 mm Hg and DBP <90 mm Hg.
exam_2013_to_2023 <- exam_2013_to_2023 %>%
  mutate(bp_control = ifelse(BPXSY < 140 & BPXDI < 90, 1, 0))

head(exam_2013_to_2023)
save(exam_2013_to_2023, file = '/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/exam_2013_to_2023.RData')
```

# Sanity check
```{r}
rm(list = ls())
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/exam_2013_to_2023.RData')
load("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/demo_1999_to_2023.RData")
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/Lab_df.RData')
```

# Xgboost/RF based feature selection
```{r}
rm(list = ls())
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/dat_train_test_bootstrapped_2013_2020.RData')

extract_feature_importance <- function(model, model_type = c("gb", "rf")) {
  # Xgboost/RF based feature selection
  # feature importance calculation:
  # - xgb$level[[1]]$frame shows var, id, and loss
  # - rf$tree[[i]]$frame shows i-th tree's var, id, loss, etc. 
  # - if parent id is i, child nodes' ids are defined as i*2, i*2+1, i*2+2, etc. 
  # - this means that parent ids are powers of 2's 
  # - loss is based on MSE loss (jasa)
  # - define loss reduction at node i = parent of node i's loss - node i's loss
  # - accumulate all loss reduction grouped by the variables 
  # Basically, more loss reduction means more feature important

  # Ensure valid model_type input
  model_type <- match.arg(model_type)

  # Handle RPMS Gradient Boosting (GB)
  if (model_type == "gb") {
    if (!is.data.frame(model$level[[1]]$frame)) {
      stop("Invalid RPMS Boost model: Missing frame data.")
    }
    
    model_frame <- model$level[[1]]$frame

    # Remove "Root" node
    model_frame <- model_frame %>% filter(var != "Root")

    # Compute Parent Node ID
    model_frame <- model_frame %>%
      mutate(Parent_Node = ifelse(node == 1, NA, floor(node / 2)))

    # Merge parent node loss
    feature_importance <- model_frame %>%
      left_join(model_frame %>% select(node, loss), by = c("Parent_Node" = "node"), suffix = c("", "_parent")) %>%
      mutate(Loss_Reduction = loss_parent - loss) %>%
      group_by(var) %>%
      summarize(Total_Loss_Reduction = sum(Loss_Reduction, na.rm = TRUE)) %>%
      arrange(desc(Total_Loss_Reduction)) %>%
      rename(Feature = var)

    plot_title <- "Feature Importance in RPMS Boosting (GB)"
    
  } else if (model_type == "rf") {
    # Handle RPMS Random Forest (RF)
    if (!is.list(model$tree)) {
      stop("Invalid RPMS Random Forest model: Missing tree list.")
    }
    
    # Extract all trees' frames
    all_frames <- lapply(model$tree, function(tree) tree$frame)

    # Combine all frames into one large data frame
    model_frame <- bind_rows(all_frames, .id = "Tree_ID")

    # Remove "Root" node
    model_frame <- model_frame %>% filter(var != "Root")

    # Compute Parent Node ID
    model_frame <- model_frame %>%
      mutate(Parent_Node = ifelse(node == 1, NA, floor(node / 2)))

    # Merge parent node loss
    feature_importance <- model_frame %>%
      left_join(model_frame %>% select(node, loss), by = c("Parent_Node" = "node"), suffix = c("", "_parent")) %>%
      mutate(Loss_Reduction = loss_parent - loss) %>%
      group_by(var) %>%
      summarize(Total_Loss_Reduction = sum(Loss_Reduction, na.rm = TRUE)) %>%
      arrange(desc(Total_Loss_Reduction)) %>%
      rename(Feature = var)

    plot_title <- "Feature Importance in RPMS Random Forest (RF)"
  }

  #print(feature_importance)
  return(feature_importance) 
}

# Function to extract selected features from one imputed dataset
run_model_and_extract_features <- function(data, outcome_var, vars, mode = c("gb", "rf")) {
  # Define survey-related variables to exclude
  survey_vars <- c("svy_strata", "svy_weight_mec", "svy_psu")

  # Remove survey-related variables from vars
  vars <- setdiff(vars, survey_vars)

  # Construct formula dynamically
  formula_str <- paste(outcome_var, "~", paste(vars, collapse = " + "))
  formula_obj <- as.formula(formula_str)

  # Match mode input
  mode <- match.arg(mode)

  # Train the model (either "gb" or "rf")
  if (mode == "gb") {
    model <- rpms_boost(
      formula_obj, 
      data = data, 
      strata = ~svy_strata, 
      weights = ~svy_weight_mec, 
      clusters = ~svy_psu, 
      pval = 0.1
    )
  } else if (mode == "rf") {
    model <- rpms_forest(
      formula_obj, 
      data = data, 
      strata = ~svy_strata, 
      weights = ~svy_weight_mec, 
      clusters = ~svy_psu, 
      f_size = 100, 
      cores = 5
    )
  }
  # Extract feature importance based on mode
  importance_df <- extract_feature_importance(model, model_type = mode)

  # Return feature importance data frame
  return(importance_df)
}
# Function to apply majority voting on feature lists
majority_vote_features <- function(feature_lists, vars, n = 25) {
  # Count occurrences of each feature
  feature_counts <- table(unlist(feature_lists))  
  feature_df <- as.data.frame(feature_counts)  # Convert to data frame

  # Convert Feature column to character (Fixes factor issue)
  feature_df$Var1 <- as.character(feature_df$Var1) 
  colnames(feature_df) <- c("Feature", "Count")  # Rename columns

  # Filter only features that exist in vars
  feature_df <- feature_df[feature_df$Feature %in% vars, ]

  # Sort by count (highest first) and return top n features
  feature_df <- feature_df[order(-feature_df$Count), ]  # Descending sort
  feature_df <- head(feature_df, n)  # Select top n features

  return(feature_df)
}


# CODE FOR RUNNING for a given year (e.g. dat_hyp_2013_samp)
# need to remove special characters because rpms is stupid 
clean_variable_names <- function(data) {
  clean_names <- function(names_vec) {
    names_vec <- gsub("[^A-Za-z0-9_]", "_", names_vec)  # Replace all non-alphanumeric characters with "_"
    names_vec <- gsub("_+", "_", names_vec)  # Remove duplicate underscores
    names_vec <- gsub("^_|_$", "", names_vec)  # Remove leading/trailing underscores
    return(names_vec)
  }

  if (is.data.frame(data)) {
    # If input is a dataframe, clean its column names
    names(data) <- clean_names(names(data))
    return(data)
  } else if (is.list(data)) {
    # If input is a list of strings, clean each element
    return(lapply(data, clean_names))
  } else if (is.character(data)) {
    # If input is a vector of column names, clean directly
    return(clean_names(data))
  } else {
    stop("Input must be a dataframe, a list of character vectors, or a character vector.")
  }
}
data_list <- list(
  #"2013" = dat_hyp_2013_samp,
  #"2015" = dat_hyp_2015_samp,
  "2017" = dat_hyp_2017_samp[1:2]
)

# Loop through each dataset year
for (year in names(data_list)) {
  cat("\nProcessing Year:", year, "\n")
  
  # Select dataset
  dataset <- data_list[[year]]
  
  # Define exclusion list (already modified based on your previous request)
  columns_to_exclude <- clean_variable_names(columns_to_exclude <- c(
  'bp_control_jnc7', 'bp_control_accaha', 'SEQN', 'bp_med_recommended_accaha.Yes', 'bp_med_recommended_jnc7.Yes', 
  'htn_resistant_accaha.Yes', 'svy_strata', 'svy_weight_mec', 'svy_psu', 
  'htn_resistant_jnc7.Yes', 'htn_aware.Yes', 'bp_med_use.Yes',
  'bp_med_n_class.None', 'bp_med_n_class.One', 'bp_med_n_class.Two',
  'bp_med_n_class.Three', 'bp_med_n_class.Four or more', 'demo_age_cat.18 to 44', 'demo_age_cat.45 to 64',
  'demo_age_cat.65 to 74', 'demo_age_cat.75+', 'bp_med_alpha.Yes', 'bp_med_n_pills.None', 'bp_med_n_pills.One', 'bp_med_n_pills.Two',
  'bp_med_n_pills.Three', 'bp_med_n_pills.Four or more', 
  'bp_med_diur_Ksparing.Yes', 'bp_med_vasod.Yes', 'bp_med_ccb_ndh.Yes', 'bp_med_ccb_dh.Yes', 'cc_cvd_hf.Yes', 'bp_med_aldo.Yes', 'cc_egfr_lt60.Yes',
  'bp_med_ace.Yes', 'bp_med_angioten.Yes', 'bp_med_beta.Yes', 'bp_med_ccb.Yes',
  'bp_med_central.Yes', 'bp_med_renin_inhibitors.Yes', 'bp_med_diur_loop.Yes',
  'bp_med_diur_thz.Yes', 'WHD010', 'WHD050', 'WHD020'
))
  print(columns_to_exclude)
  
  # Clean variable names
  dataset <- lapply(dataset, function(boot_sample) {
    lapply(boot_sample, clean_variable_names)
  })

  # Define feature selection set
  all_vars <- colnames(dataset[[1]][[1]])
  vars_for_jnc7 <- setdiff(all_vars, columns_to_exclude)
  print(vars_for_jnc7)
  vars_for_accaha <- setdiff(all_vars, columns_to_exclude)

  # Set parameters
  outcome_var <- "bp_control_jnc7"  # Change to "bp_control_accaha" if needed
  vars <- vars_for_jnc7  # Change to vars_for_accaha if needed
  mode <- "gb"  # Change to "rf" for random forest

  # Run feature selection on all 5 imputed datasets for each bootstrap sample
  bootstrap_selected_features <- lapply(dataset, function(boot_sample) {
    imputed_feature_sets <- lapply(boot_sample, function(data) {
      tryCatch({
        # Convert outcome variable to numeric
        data[[outcome_var]] <- as.numeric(data[[outcome_var]])
        
        # Run feature selection
        run_model_and_extract_features(data, outcome_var, vars, mode=mode)
        
      }, error = function(e) {
        cat("Skipping dataset due to error:", conditionMessage(e), "\n")
        return(NULL)  # Return NULL if error occurs
      })
    })
    
    # Remove failed runs (NULL values)
    imputed_feature_sets <- Filter(Negate(is.null), imputed_feature_sets)
    
    # Get the top 25 features from successful datasets
    if (length(imputed_feature_sets) > 0) {
      majority_vote_features(imputed_feature_sets, vars, n = 25)
    } else {
      cat("Skipping bootstrap sample due to all failures.\n")
      return(NULL)  # Skip this bootstrap if all 5 datasets fail
    }
  })

  # Remove failed bootstrap samples
  bootstrap_selected_features <- Filter(Negate(is.null), bootstrap_selected_features)

  # Perform majority voting over bootstrapped datasets (returning top 25)
  print(bootstrap_selected_features)
  final_selected_features <- majority_vote_features(bootstrap_selected_features, vars, n = 25)

  # Save the final selected features to CSV
  output_filename <- paste0("final_features_for_", mode, "_", year, ".csv")
  write.csv(final_selected_features, output_filename, row.names = FALSE)

  # Print the final selected features
  cat("\nFinal selected features for", year, "saved as", output_filename, "\n")
  print(final_selected_features)
}


# Now evaluate 
data_list <- list(
  "2013" = list(train = dat_hyp_2013_std_train, test = dat_hyp_2013_std_test),
  "2015" = list(train = dat_hyp_2015_std_train, test = dat_hyp_2015_std_test),
  "2017" = list(train = dat_hyp_2017_std_train, test = dat_hyp_2017_std_test)
)

# Define outcome variable
outcome_var <- "bp_control_jnc7"

# Loop through each dataset year
for (year in names(data_list)) {
  cat("\nProcessing Year:", year, "\n")
  
  # Load the correct feature set
  features_path <- paste0(
    "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/R/final_features_for_gb_", 
    year, ".csv"
  )
  
  if (!file.exists(features_path)) {
    cat("Skipping", year, "- Feature file not found:", features_path, "\n")
    next
  }
  
  selected_features <- read.csv(features_path, stringsAsFactors = FALSE)$Feature

  # Retrieve training and test data
  dat_train <- data_list[[year]]$train
  dat_test <- data_list[[year]]$test

  # Apply variable name cleaning
  dat_train <- lapply(dat_train, clean_variable_names)
  dat_test <- lapply(dat_test, clean_variable_names)

  # Train models on 5 imputed datasets
  xgb_models <- lapply(dat_train, function(train_data) {
    tryCatch({
      # Convert outcome variable to numeric
      train_data[[outcome_var]] <- as.numeric(train_data[[outcome_var]])

      # Define formula dynamically
      formula_obj <- as.formula(paste(outcome_var, "~", paste(selected_features, collapse = " + ")))

      # Train XGBoost model with survey weights
      xgb <- rpms_boost(
        formula_obj, 
        data = train_data, 
        strata = ~svy_strata, 
        weights = ~svy_weight_mec, 
        clusters = ~svy_psu, 
        pval = 0.05
      )
      return(xgb)
    }, error = function(e) {
      cat("Skipping dataset due to error:", conditionMessage(e), "\n")
      return(NULL)  # Skip failed datasets
    })
  })

  # Remove failed runs (NULL values)
  xgb_models <- Filter(Negate(is.null), xgb_models)

  # Function to convert predictions to binary (0/1)
  binarize_predictions <- function(preds) {
    ifelse(preds >= 1.5, 2, 1)
  }

  # Initialize list to store AUC values
  auc_values <- vector("numeric", length(dat_test))

  # Loop over 5 imputed test datasets
  for (i in seq_along(dat_test)) {
    if (is.null(xgb_models[[i]])) {
      cat("Skipping test set", i, "due to missing trained model.\n")
      next
    }
    
    test_data <- dat_test[[i]]

    # Convert outcome variable to numeric
    test_data[[outcome_var]] <- as.numeric(test_data[[outcome_var]])

    # Get predictions
    preds <- predict(xgb_models[[i]], newdata = test_data)

    # Convert to binary classification
    preds_binary <- binarize_predictions(preds)

    # Compute AUC
    auc_values[i] <- auc(roc(test_data$bp_control_accaha, preds_binary))
  }

  # Remove missing AUCs
  auc_values <- auc_values[!is.na(auc_values)]

  # Compute mean AUC over the test datasets
  if (length(auc_values) > 0) {
    mean_auc <- mean(auc_values)
    sd_auc <- sd(auc_values)
    cat("\nMean AUC/ROC for", year, ":", round(mean_auc, 4), "\n")
    cat("SD AUC/ROC for", year, ":", round(sd_auc, 4), "\n")
  } else {
    cat("\nSkipping AUC calculation for", year, "due to missing predictions.\n")
  }
}
```
# 2021-2023 data creation
```{r}
rm(list = ls())

load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/auc_values_ES.RData')
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/top_features_ES.RData')
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/dat_hyp_final_2013_2020.RData')

clean_names <- function(features) {
  sub("\\..*", "", features)
}

# Apply the cleaning function to each feature list
cleaned_2013 <- clean_names(top_features_2013)
cleaned_2015 <- clean_names(top_features_2015)
cleaned_2017 <- clean_names(top_features_2017)

# Find the intersection of the cleaned variable names
common_features <- intersect(intersect(cleaned_2013, cleaned_2015), cleaned_2017)
union_features <- union(union(cleaned_2013, cleaned_2015), cleaned_2017)

data_dir <- "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data"
xpt_files <- list.files(data_dir, pattern = "\\.xpt$", recursive = TRUE, full.names = TRUE)
matching_files <- c()
for (file in xpt_files) {
  # Read the .xpt file
  df <- tryCatch(read_xpt(file), error = function(e) NULL)
  
  # Skip files that could not be read
  if (is.null(df)) next
  
  # Check if any variable in union_features exists in the dataset
  matched_vars <- intersect(names(df), union_features)
  
  if (length(matched_vars) > 0) {
    matching_files[[file]] <- matched_vars  # Store the matched variables
  }
}
print(matching_files) # the ones i don't need to recode 

files_needed <- c("DEMO_L.xpt", "BMX_L.xpt", "BPXO_L.xpt", "INQ_L.xpt", "HDL_L.xpt", "WHQ_L.xpt", "MCQ_L.xpt", "SMQ_L.xpt", "DIQ_L.xpt", "CBC_L.xpt", "PBCD_L.xpt", "TCHOL_L.xpt", "HIQ_L.xpt", "KIQ_U_L.xpt", "OHQ_L.xpt") 
xpt_files <- list.files(path = data_dir, pattern = "\\.xpt$", recursive = TRUE, full.names = TRUE)
xpt_files <- xpt_files[basename(xpt_files) %in% files_needed]

datasets <- lapply(xpt_files, function(file) {
  df <- read_xpt(file) %>% as.data.frame()
  return(df)
})

data <- Reduce(function(x, y) inner_join(x, y, by = "SEQN"), datasets)

data <- data %>%
  mutate(
    svy_psu = SDMVPSU,
    svy_strata = SDMVSTRA,
    svy_weight_mec = WTMEC2YR,
    
    demo_race = case_when(
      RIDRETH3 == 3 ~ "Non-Hispanic White",
      RIDRETH3 == 4 ~ "Non-Hispanic Black",
      RIDRETH3 == 6 ~ "Non-Hispanic Asian",
      RIDRETH3 %in% c(1, 2) ~ "Hispanic",
      RIDRETH3 == 7 ~ "Other",
      TRUE ~ NA_character_
    ) %>% factor(levels = c("Non-Hispanic White", "Non-Hispanic Black", "Non-Hispanic Asian", "Hispanic", "Other")),
    
    demo_gender = factor(RIAGENDR, labels = c("Men", "Women")),
    demo_age_years = as.numeric(RIDAGEYR),
    
    weight_change = as.numeric(WHD020) - as.numeric(WHD050),
    
    BPXSY = rowMeans(select(., BPXOSY1, BPXOSY2, BPXOSY3), na.rm = TRUE) + 1.5, 
    BPXDI = rowMeans(select(., BPXODI1, BPXODI2, BPXODI3), na.rm = TRUE) - 1.3, 
    
    bp_control_jnc7 = factor(ifelse(BPXSY < 140 & BPXDI < 90, "Yes", "No"), levels = c("No", "Yes")),
    
    cc_cvd_any = case_when(
      if_any(c(MCQ160E, MCQ160F, MCQ160C, MCQ160B), ~ . == 1) ~ "Yes",
      if_all(c(MCQ160E, MCQ160F, MCQ160C, MCQ160B), ~ . == 2) ~ "No",
      TRUE ~ NA_character_
    ) %>% factor(levels = c("No", "Yes")),
    
    cc_diabetes = case_when(
      DIQ070 == 1 | DIQ010 == 1 ~ "Yes",
      DIQ070 == 2 | DIQ010 %in% c(2,3) ~ "No",
      TRUE ~ NA_character_
    ) %>% factor(levels = c("No", "Yes")),
    
    cc_smoke = case_when(
      SMQ040 %in% c(1, 2) ~ "Current",  # If SMQ040 is 1 or 2 → "Current"
      SMQ040 %in% c(2, 3) & SMQ020 == 1 ~ "Former",  # If SMQ040 is 2 or 3 AND SMQ020 == 1 → "Former"
      SMQ040 == 3 & SMQ020 == 2 ~ "Never",  # If SMQ040 is 3 AND SMQ020 == 2 → "Never"
      SMQ020 == 2 ~ "Never", # SMQ040 has a lot missing, so 
      TRUE ~ NA_character_  # Assign NA if none of the conditions match
    ) %>% factor(levels = c("Never", "Former", "Current"))
  ) %>%
  select(
    SEQN, BMXBMI, LBXRBCSI, LBXBPB, LBXTHG, LBXTC, HIQ011, KIQ022, OHQ845, LBDMONO,
    svy_psu, svy_strata, svy_weight_mec, demo_race, demo_gender, demo_age_years, 
    weight_change, bp_control_jnc7, cc_cvd_any, cc_diabetes, cc_smoke
  )

# Check missingness
missing_summary <- data %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  arrange(desc(Missing_Count))

print(missing_summary)

save(data, file = "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/dat_unimputed_2021_2023.RData")
```





