---
title: "ws"
output: html_document
date: "2025-02-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TODO:
https://docs.google.com/document/d/1IYLTEWNzi34iagvPAwtEt4nIZdpzWqoQ5zt4LkySbhg/edit?tab=t.0 
1. merge 2021-2023 data with original exam data 
2. extract my variables 2013-2023 and demographics variables with SEQN
3. recode variables based on excel logic, document everything
4. save RData and uplaod to github

```{r}
# load packages here
library(haven)
library(dplyr)
library(rpms)
library(ggplot2)
library(pROC)
source('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/R/funcs.R')
```

# Load 1999-2020 exam data (nhanesA format)
```{r}
rm(list = ls())
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/Examinations.RData')
```

# Save as dataframe with 1999-2020 data
```{r}
exnames <- c("AUX", "BPX", "BMX", 'DXX', 'BPXO', 'OHXDENT')

ex_dat <- ex_dat[names(ex_dat) %in% exnames]

ex_df <- combine_surveys(ex_dat)

save(ex_df, file = "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/Examinations_1999_to_2020.RData")
```

# Load 2021-2023 data for exam and demographics
```{r}
rm(list = ls())

load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/Examinations_1999_to_2020.RData')
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/Demographics.RData')
demo_df <- demo_dat$DEMO

BAX_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/exam/BAX_L.xpt")
BMX_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/exam/BMX_L.xpt")
BPXO_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/exam/BPXO_L.xpt")
LUX_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/exam/LUX_L.xpt")
DEMO_L <- read_xpt("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/2021_to_2023_data/demographics/DEMO_L.xpt")
```

# Merge to create 1999-2023 data
```{r}
# Add Year = 2021 to all smaller datasets
datasets <- list(BAX_L, BMX_L, BPXO_L, LUX_L)

datasets <- lapply(datasets, function(df) {
  df <- as.data.frame(df)  # Ensure it's a data frame
  df <- df %>% mutate(Year = '2021')  # Add Year
  return(df)
})

# Perform full joins across all datasets
full_joined_data <- Reduce(function(x, y) full_join(x, y, by = c("SEQN", "Year")), datasets)

# Bind ex_df with the fully joined dataset
ex_df_joined <- bind_rows(ex_df, full_joined_data)


# Merge Demo data
datasets <- list(DEMO_L)
datasets <- lapply(datasets, function(df) {
  df <- as.data.frame(df)  # Ensure it's a data frame
  df <- df %>% mutate(Year = '2021')  # Add Year
  return(df)
})
demo_df_joined <- bind_rows(demo_df, datasets)
duplicate_seqn <- demo_df_joined %>%
  group_by(SEQN, Year) %>%
  filter(n() > 1)  # Keep only duplicates

# Display duplicate SEQN count
cat("Number of duplicated SEQN values:", nrow(duplicate_seqn), "\n")


# Saving 
demo_1999_to_2023 <- demo_df_joined
exam_1999_to_2023 <- ex_df_joined
save(demo_1999_to_2023, file = "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/demo_1999_to_2023.RData")
save(exam_1999_to_2023, file = "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/exam_1999_to_2023.RData")
```

# Extract necessary variables from the 1999-2023 data
```{r}
rm(list = ls())
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/exam_1999_to_2023.RData')

exvars <- c(
  # ID, YEAR
  "SEQN", "Year",
  
  # BPX (Blood Pressure Exam)
  "BPQ150A", "BPQ150B", "BPQ150C", "BPQ150D", 
  "BPXPULS", "BPXPTY", "BPXML1", 
  "BPXSY1", "BPXDI1", "BPAEN1", "BPXSY2", "BPXDI2", "BPAEN2", 
  "BPXSY3", "BPXDI3", "BPAEN3", "BPXSY4", "BPXDI4", "BPAEN4",
  "BPXOSY1", "BPXODI1", 
  "BPXOSY2", "BPXODI2", 
  "BPXOSY3", "BPXODI3", 
  "BPXOPLS1", "BPXOPLS2", "BPXOPLS3",
  
  # BMX (Body Measurements Exam)
  "BMXBMI", "BMXWAIST", "BMXHIP",
  
  # DXX (Dual-energy X-ray Absorptiometry - DXA Scan)
  "DXXLSBMD", "DXDTRBMD", "DXDTRPF", "DXXLABMD", 
  "DXDLAPF", "DXXRABMD", "DXDRAPF",
  
  # AUX (Audiometry Exam)
  "AUAEXSTS", "AUQ020", "AUQ030", 
  "AUXU1K1R", "AUXU500R", "AUXU1K2R", "AUXU2KR", "AUXU3KR", 
  "AUXU4KR", "AUXU6KR", "AUXU8KR", "AUXU1K1L", "AUXU500L", 
  "AUXU1K2L", "AUXU2KL", "AUXU3KL", "AUXU4KL", "AUXU6KL", "AUXU8KL",
  "AUXR1K1R", "AUXR5CR", "AUXR1K2R", "AUXR2KR", "AUXR3KR", "AUXR4KR", 
  "AUXR6KR", "AUXR8KR", "AUXR1K1L", "AUXR5CL", "AUXR1K2L", "AUXR2KL", 
  "AUXR3KL", "AUXR4KL", "AUXR6KL", "AUXR8KL",
  
  # OHX (Oral Health Exam - Dental)
  "OHDEXSTS", "OHAREC", "OHAROCDT", "OHAROCGP", "OHAROCOH", 
  "OHAROCCI", "OHAROCDE",
  
  # BMDAVSAD (Bone Mineral Density - DXA Scan)
  "BMDAVSAD"
)

exam_subset <- exam_1999_to_2023 %>%
  filter(Year %in% c("2013", "2015", "2017", "P", "2021")) %>%
  select(any_of(exvars))  

missingness <- exam_subset %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "Missing_Percentage")
print(missingness)

```
# Data re-coding
```{r}
# Define BP variables for general years and 2021
bp_vars_general <- c("BPXSY1", "BPXDI1", "BPXSY2", "BPXDI2", "BPXSY3", "BPXDI3", "BPXSY4", "BPXDI4")
bp_vars_2021 <- c("BPXOSY1", "BPXODI1", "BPXOSY2", "BPXODI2", "BPXOSY3", "BPXODI3")
# Compute mean and standard deviation separately for 2021 and other years
bp_stats_general <- exam_subset %>%
  filter(Year != 2021) %>%
  summarise(across(all_of(bp_vars_general), list(mean = ~ mean(. , na.rm = TRUE),
                                                 sd = ~ sd(. , na.rm = TRUE)))) %>%
  pivot_longer(cols = everything(), names_to = c("Variable", ".value"), names_sep = "_") %>%
  mutate(Year = "Other Years")

bp_stats_2021 <- exam_subset %>%
  filter(Year == 2021) %>%
  summarise(across(all_of(bp_vars_2021), list(mean = ~ mean(. , na.rm = TRUE),
                                              sd = ~ sd(. , na.rm = TRUE)))) %>%
  pivot_longer(cols = everything(), names_to = c("Variable", ".value"), names_sep = "_") %>%
  mutate(Year = "2021")
# Combine both statistics
bp_stats_long <- bind_rows(bp_stats_general, bp_stats_2021)
# Display the results
print(bp_stats_long)


# Since not much fluctuation between BP1-BP3, we will take average of 1,2,3 to create new numerical variables called BPXSY and BPXDI
# Dual-Energy X-ray 
# select DXDTRBMD, DXXLABMD based on low correlation with other bmi variables and DXD 
# 2021 and P is measured by BPXOSY, BPXODI
exam_bp <- exam_subset %>%
  mutate(
    BPXSY = ifelse(Year %in% c('2021', 'P'), 
                   rowMeans(select(., BPXOSY1, BPXOSY2, BPXOSY3), na.rm = TRUE) + 1.5, 
                   rowMeans(select(., BPXSY1, BPXSY2, BPXSY3), na.rm = TRUE)),
    BPXDI = ifelse(Year %in% c('2021', 'P'), 
                   rowMeans(select(., BPXODI1, BPXODI2, BPXODI3), na.rm = TRUE) - 1.3, 
                   rowMeans(select(., BPXDI1, BPXDI2, BPXDI3), na.rm = TRUE))
  ) %>%
  select(SEQN, Year, BPXSY, BPXDI, BPXPULS, BMXBMI, DXDTRBMD, DXXLABMD)  # Keep only relevant columns

# removing BMXWAIST since high correlation with BMXBMI
print(cor.test(exam_subset$BMXBMI, exam_subset$BMXWAIST))

missingness <- exam_bp %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "Missing_Percentage")
print(missingness)

# Encode into hearing loss vs noraml based on literature's threshold for normal hearing 
exam_audio <- exam_subset %>%
  rowwise() %>%  
  mutate(hearing_loss = ifelse(
    any(c(AUXU1K1R, AUXU500R, AUXU1K2R, AUXU2KR, AUXU3KR, 
          AUXU4KR, AUXU6KR, AUXU8KR, AUXU1K1L, AUXU500L, 
          AUXU1K2L, AUXU2KL, AUXU3KL, AUXU4KL, AUXU6KL, AUXU8KL) > 21, 
        na.rm = TRUE), 
    "No",   # Normal
    "Yes"   # Hearing Loss
  )) %>%
  select(SEQN, Year, hearing_loss)
table(exam_audio$hearing_loss)


exam_2013_to_2023 <- exam_bp %>%
  left_join(exam_audio, by = c("SEQN", "Year"))

# Define BP control defined as SBP <140 mm Hg and DBP <90 mm Hg.
exam_2013_to_2023 <- exam_2013_to_2023 %>%
  mutate(bp_control = ifelse(BPXSY < 140 & BPXDI < 90, 1, 0))

head(exam_2013_to_2023)
save(exam_2013_to_2023, file = '/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/exam_2013_to_2023.RData')
```

# Sanity check
```{r}
rm(list = ls())
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/exam_2013_to_2023.RData')
load("/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/1999_to_2023_original/demo_1999_to_2023.RData")
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/Lab_df.RData')
```

# Xgboost/RF based feature selection
```{r}
rm(list = ls())
load('/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/dat_train_test_bootstrapped_2013_2020.RData')

extract_feature_importance <- function(model, model_type = c("gb", "rf")) {
  # Xgboost/RF based feature selection
  # feature importance calculation:
  # - xgb$level[[1]]$frame shows var, id, and loss
  # - rf$tree[[i]]$frame shows i-th tree's var, id, loss, etc. 
  # - if parent id is i, child nodes' ids are defined as i*2, i*2+1, i*2+2, etc. 
  # - this means that parent ids are powers of 2's 
  # - loss is based on MSE loss (jasa)
  # - define loss reduction at node i = parent of node i's loss - node i's loss
  # - accumulate all loss reduction grouped by the variables 
  # Basically, more loss reduction means more feature important

  # Ensure valid model_type input
  model_type <- match.arg(model_type)

  # Handle RPMS Gradient Boosting (GB)
  if (model_type == "gb") {
    if (!is.data.frame(model$level[[1]]$frame)) {
      stop("Invalid RPMS Boost model: Missing frame data.")
    }
    
    model_frame <- model$level[[1]]$frame

    # Remove "Root" node
    model_frame <- model_frame %>% filter(var != "Root")

    # Compute Parent Node ID
    model_frame <- model_frame %>%
      mutate(Parent_Node = ifelse(node == 1, NA, floor(node / 2)))

    # Merge parent node loss
    feature_importance <- model_frame %>%
      left_join(model_frame %>% select(node, loss), by = c("Parent_Node" = "node"), suffix = c("", "_parent")) %>%
      mutate(Loss_Reduction = loss_parent - loss) %>%
      group_by(var) %>%
      summarize(Total_Loss_Reduction = sum(Loss_Reduction, na.rm = TRUE)) %>%
      arrange(desc(Total_Loss_Reduction)) %>%
      rename(Feature = var)

    plot_title <- "Feature Importance in RPMS Boosting (GB)"
    
  } else if (model_type == "rf") {
    # Handle RPMS Random Forest (RF)
    if (!is.list(model$tree)) {
      stop("Invalid RPMS Random Forest model: Missing tree list.")
    }
    
    # Extract all trees' frames
    all_frames <- lapply(model$tree, function(tree) tree$frame)

    # Combine all frames into one large data frame
    model_frame <- bind_rows(all_frames, .id = "Tree_ID")

    # Remove "Root" node
    model_frame <- model_frame %>% filter(var != "Root")

    # Compute Parent Node ID
    model_frame <- model_frame %>%
      mutate(Parent_Node = ifelse(node == 1, NA, floor(node / 2)))

    # Merge parent node loss
    feature_importance <- model_frame %>%
      left_join(model_frame %>% select(node, loss), by = c("Parent_Node" = "node"), suffix = c("", "_parent")) %>%
      mutate(Loss_Reduction = loss_parent - loss) %>%
      group_by(var) %>%
      summarize(Total_Loss_Reduction = sum(Loss_Reduction, na.rm = TRUE)) %>%
      arrange(desc(Total_Loss_Reduction)) %>%
      rename(Feature = var)

    plot_title <- "Feature Importance in RPMS Random Forest (RF)"
  }

  #print(feature_importance)
  return(feature_importance) 
}

# Function to extract selected features from one imputed dataset
run_model_and_extract_features <- function(data, outcome_var, vars, mode = c("gb", "rf")) {
  # Define survey-related variables to exclude
  survey_vars <- c("svy_strata", "svy_weight_mec", "svy_psu")

  # Remove survey-related variables from vars
  vars <- setdiff(vars, survey_vars)

  # Construct formula dynamically
  formula_str <- paste(outcome_var, "~", paste(vars, collapse = " + "))
  formula_obj <- as.formula(formula_str)

  # Match mode input
  mode <- match.arg(mode)

  # Train the model (either "gb" or "rf")
  if (mode == "gb") {
    model <- rpms_boost(
      formula_obj, 
      data = data, 
      strata = ~svy_strata, 
      weights = ~svy_weight_mec, 
      clusters = ~svy_psu, 
      pval = 0.05
    )
  } else if (mode == "rf") {
    model <- rpms_forest(
      formula_obj, 
      data = data, 
      strata = ~svy_strata, 
      weights = ~svy_weight_mec, 
      clusters = ~svy_psu, 
      f_size = 100, 
      cores = 5
    )
  }
  # Extract feature importance based on mode
  importance_df <- extract_feature_importance(model, model_type = mode)

  # Return feature importance data frame
  return(importance_df)
}
# Function to apply majority voting on feature lists
majority_vote_features <- function(feature_lists, vars, n = 25) {
  # Count occurrences of each feature
  feature_counts <- table(unlist(feature_lists))  
  feature_df <- as.data.frame(feature_counts)  # Convert to data frame

  # Convert Feature column to character (Fixes factor issue)
  feature_df$Var1 <- as.character(feature_df$Var1) 
  colnames(feature_df) <- c("Feature", "Count")  # Rename columns

  # Filter only features that exist in vars
  feature_df <- feature_df[feature_df$Feature %in% vars, ]

  # Sort by count (highest first) and return top n features
  feature_df <- feature_df[order(-feature_df$Count), ]  # Descending sort
  feature_df <- head(feature_df, n)  # Select top n features

  return(feature_df)
}


# CODE FOR RUNNING for a given year (e.g. dat_hyp_2013_samp)
# need to remove special characters because rpms is stupid 
clean_variable_names <- function(df) {
  names(df) <- gsub("[^A-Za-z0-9_]", "_", names(df))  # Replace all non-alphanumeric characters with "_"
  names(df) <- gsub("_+", "_", names(df))  # Remove duplicate underscores
  names(df) <- gsub("^_|_$", "", names(df))  # Remove leading/trailing underscores
  return(df)
}

# PARAMETERS THAT NEED TO BE CHANGED:
# 1. data (e.g. dat_hyp_2013_samp)
# 2. outcome_var (bp_control_jnc7 or bp_control accaha)
# 3. vars (either vars_for_jnc7 or vars_for_accaha)
# 4. mode (e.g. rf for random forest,  gb for gradient boost)
dat_hyp_2015_samp <- lapply(dat_hyp_2015_samp, function(boot_sample) {
  lapply(boot_sample, clean_variable_names)
})
all_vars <- colnames(dat_hyp_2015_samp[[1]][[1]])
vars_for_jnc7 <- setdiff(all_vars, c('bp_control_jnc7', 'bp_control_accaha', 'SEQN', 'bp_med_recommended_accaha', 'htn_resistant_accaha.Yes'))
vars_for_accaha <- setdiff(all_vars, c('bp_control_jnc7', 'bp_control_accaha', 'SEQN', 'bp_med_recommended_jnc7', 'htn_resistant_jnc7.Yes'))

outcome_var <- "bp_control_jnc7"
vars <- vars_for_jnc7 
mode <- 'gb'

# Run feature selection on all 5 imputed datasets for each bootstrap sample
bootstrap_selected_features <- lapply(dat_hyp_2015_samp, function(boot_sample) {
  imputed_feature_sets <- lapply(boot_sample, function(data) {
    tryCatch({
      # Convert outcome variable to numeric
      data[[outcome_var]] <- as.numeric(data[[outcome_var]])
      
      # Run feature selection
      run_model_and_extract_features(data, outcome_var, vars, mode=mode)
      
    }, error = function(e) {
      cat("Skipping dataset due to error:", conditionMessage(e), "\n")
      return(NULL)  # Return NULL if error occurs
    })
  })
  
  # Remove failed runs (NULL values)
  imputed_feature_sets <- Filter(Negate(is.null), imputed_feature_sets)
  
  # Get the top 25 features from successful datasets
  if (length(imputed_feature_sets) > 0) {
    majority_vote_features(imputed_feature_sets, vars, n = 25)
  } else {
    cat("Skipping bootstrap sample due to all failures.\n")
    return(NULL)  # Skip this bootstrap if all 5 datasets fail
  }
})

# Remove failed bootstrap samples
bootstrap_selected_features <- Filter(Negate(is.null), bootstrap_selected_features)

# Perform majority voting over 50 bootstrapped datasets (returning top 25)
final_selected_features <- majority_vote_features(bootstrap_selected_features, vars, n = 25)

# Save the final selected features to CSV
write.csv(final_selected_features, paste0("final_features_for_", mode, "2015.csv"), row.names = FALSE)

# Print the final selected features
print(final_selected_features)
```

# Evaluate Xgboost
```{r}
features_path <- "/Users/taehyo/Library/CloudStorage/Dropbox/NYU/Research/Research/Code/ENAR-Risk-Factors-Analyses/data/cleaned/2013_to_2023_cleaned/selected features/jnc7/final_features_for_gb2013.csv"
# Read the selected features as a vector
selected_features <- read.csv(features_path, stringsAsFactors = FALSE)$Feature

library(pROC)  # For AUC calculation

# Initialize list to store AUC values
auc_values <- vector("numeric", length(dat_hyp_2013_std_train))
# Apply variable name cleaning to all test datasets
dat_hyp_2013_std_train <- lapply(dat_hyp_2013_std_train, function(train_data) {
  clean_variable_names(train_data)
})
dat_hyp_2013_std_test <- lapply(dat_hyp_2013_std_test, function(test_data) {
  clean_variable_names(test_data)
})

# Loop over 5 imputed datasets in the training set
xgb_models <- lapply(dat_hyp_2013_std_train, function(train_data) {
  
  # Convert selected predictors to numeric (excluding survey variables)
  train_data[[outcome_var]] <- as.numeric(train_data[[outcome_var]])

  # Define formula dynamically
  formula_obj <- as.formula(paste("bp_control_jnc7 ~", paste(selected_features, collapse = " + ")))

  # Train model
  xgb <- rpms_boost(
    formula_obj, 
    data = train_data, 
    strata = ~svy_strata, 
    weights = ~svy_weight_mec, 
    clusters = ~svy_psu, 
    pval = 0.05
  )
  
  return(xgb)
})

# Function to convert predictions to binary (0/1)
binarize_predictions <- function(preds) {
  ifelse(preds >= 1.5, 2, 1)
}

# Initialize list to store AUC values
auc_values <- vector("numeric", length(dat_hyp_2013_std_test))

# Loop over 5 imputed test datasets
for (i in seq_along(dat_hyp_2013_std_test)) {
  test_data <- dat_hyp_2013_std_test[[i]]

  # Convert selected predictors to numeric
  test_data[[outcome_var]] <- as.numeric(test_data[[outcome_var]])

  # Get predictions
  preds <- predict(xgb_models[[i]], newdata = test_data)

    # Convert to binary classification
  preds_binary <- binarize_predictions(preds)

  # Compute AUC
  auc_values[i] <- auc(roc(test_data$bp_control_jnc7, preds_binary))
}

# Compute mean AUC over the 5 test datasets
mean_auc <- mean(auc_values)
print(paste("Mean AUC/ROC:", round(mean_auc, 4)))

```






