{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing\n"
      ],
      "metadata": {
        "id": "iOj4ngXEQUKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7SHdCGeQTNM",
        "outputId": "6a01748e-ab39-4b4e-aa89-abba849f170c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Preprocessing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import KBinsDiscretizer, LabelEncoder, StandardScaler, OneHotEncoder\n",
        "\n",
        "# TabNet required libraries\n",
        "!pip install pytorch_tabnet wget\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import os\n",
        "import wget\n",
        "from pathlib import Path\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from pytorch_tabnet.pretraining import TabNetPretrainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmo5JBAxQfCM",
        "outputId": "3371c6ea-d96c-4312-d76b-9e9ffb264f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_tabnet in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.23.5)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.2.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (4.66.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch_tabnet) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch_tabnet) (1.3.0)\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tabnet/dat_hyp_mice.csv')\n",
        "weights = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tabnet/weights.csv')\n",
        "\n",
        "# Sampling weights give an error for TabNet.... so just cite the following quote\n",
        "# https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280387\n",
        "# However, the error was mitigated if the F1 score was subsequently recalculated with observed outcomes from the weighted dataset\n",
        "\n",
        "\n",
        "\n",
        "exclude = ['svy_subpop_htn', 'svy_subpop_chol', 'DSD010.x', 'DRDINT.x', 'DRDINT.y', 'LBDHDDSI', 'URXUMS', 'URXCRS', 'BPQ050A', 'cc_cvd_stroke']\n",
        "labels = ['bp_control_jnc7', 'bp_control_accaha', 'bp_control_140_90', 'bp_control_130_80']\n",
        "before_2013 = [1,2,3,4,5,6] # remember not to use 1999-2000\n",
        "after_2013 = [7,8,9] # remember not to use 1999-2000\n",
        "\n",
        "\n",
        "# Exclude the specified columns\n",
        "df = df.drop(columns=exclude)\n",
        "\n",
        "# Define a threshold for what you consider a categorical variable\n",
        "threshold = 10\n",
        "\n",
        "# Find categorical columns\n",
        "categorical_columns = [col for col in df.columns if df[col].nunique() <= threshold]\n",
        "continuous_columns = [col for col in df.columns if df[col].nunique() > threshold]\n",
        "\n",
        "for col in categorical_columns:\n",
        "    # Convert the categorical columns to categorical type\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "for col in continuous_columns:\n",
        "    df[col] = df[col].astype(float) # Convert to float and coerce errors\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "codebook = {}\n",
        "\n",
        "\n",
        "'''\n",
        "Column 'svy_year':\n",
        "  0 => 1999-2000\n",
        "  1 => 2001-2002\n",
        "  2 => 2003-2004\n",
        "  3 => 2005-2006\n",
        "  4 => 2007-2008\n",
        "  5 => 2009-2010\n",
        "  6 => 2011-2012\n",
        "  7 => 2013-2014\n",
        "  8 => 2015-2016\n",
        "  9 => 2017-2020\n",
        "'''\n",
        "\n",
        "\n",
        "for col in categorical_columns:\n",
        "    if col == 'svy_psu': # skip encoding of this cateogry\n",
        "      continue\n",
        "\n",
        "    # Store the mapping of original categories to encoded labels\n",
        "    codebook[col] = {index: label for index, label in enumerate(df[col].unique())}\n",
        "\n",
        "    # Fit and transform the data to apply label encoding\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "for col in continuous_columns:\n",
        "    # Fit and transform the data to apply standard scaling\n",
        "    df[col] = standard_scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
        "\n",
        "# Print the codebook\n",
        "print(\"\\nCodebook:\")\n",
        "for col, mapping in codebook.items():\n",
        "    print(f\"Column '{col}':\")\n",
        "    for original, encoded in mapping.items():\n",
        "        print(f\"  {original} => {encoded}\")\n",
        "\n",
        "\n",
        "# Filter the data for the specified years\n",
        "df_before_2013 = df[df['svy_year'].isin(before_2013)]\n",
        "df_after_2013 = df[df['svy_year'].isin(after_2013)]\n",
        "df_all_years = df.copy()\n",
        "\n",
        "datasets = {}  # A dictionary to hold all 12 datasets\n",
        "for label in labels:\n",
        "    other_labels = [l for l in labels if l != label]\n",
        "\n",
        "    for period, df_period in zip(['before_2013', 'after_2013', 'all_years'], [df_before_2013, df_after_2013, df_all_years]):\n",
        "        # Ensure the label is present in the dataset\n",
        "        if label in df_period.columns:\n",
        "            # Create a copy of the dataframe to avoid modifying the original data\n",
        "            dataset = df_period.copy()\n",
        "\n",
        "            dataset.drop(columns=other_labels, inplace=True)\n",
        "\n",
        "            # Add the processed dataset to the dictionary\n",
        "            datasets[f'{label}_{period}'] = dataset\n",
        "\n",
        "# A dictionary to hold all the splits for each dataset\n",
        "splits = {}\n",
        "for dataset_name, df in datasets.items():\n",
        "    # Identify the target variable (column starts with 'bp_control')\n",
        "    target_col = [col for col in df.columns if col.startswith('bp_control')][0]\n",
        "\n",
        "    # get sampling weight\n",
        "    weights = df['svy_weight_mec']\n",
        "    weights = np.maximum(weights, 0)\n",
        "\n",
        "    # Define the features and target\n",
        "    X = df.drop(columns=[target_col, 'svy_weight_mec'])\n",
        "    y = df[target_col]\n",
        "    feat_list = X.columns\n",
        "\n",
        "    # First split: Separate 20% of the data as test set\n",
        "    X_temp, X_test, y_temp, y_test, w_temp, w_test = train_test_split(X, y, weights, test_size=0.1)\n",
        "\n",
        "    # Adjust the ratio for the second split (0.125 x 0.8 = 0.1)\n",
        "    # This will split the remaining data into 80% train and 20% validation, which corresponds to 10% of the original data\n",
        "    X_train, X_valid, y_train, y_valid, w_train, w_valid = train_test_split(X_temp, y_temp, w_temp, test_size=1/9)\n",
        "\n",
        "    # Store the splits in the dictionary\n",
        "    splits[dataset_name] = {\n",
        "      'train': (X_train.to_numpy(), y_train.to_numpy(), w_train, feat_list),\n",
        "      'valid': (X_valid.to_numpy(), y_valid.to_numpy(), w_valid),\n",
        "      'test': (X_test.to_numpy(), y_test.to_numpy(), w_test)\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrmfy13zp3-y",
        "outputId": "0b251047-b846-4270-9eea-f45054b817ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Codebook:\n",
            "Column 'svy_year':\n",
            "  0 => 1999-2000\n",
            "  1 => 2001-2002\n",
            "  2 => 2003-2004\n",
            "  3 => 2005-2006\n",
            "  4 => 2007-2008\n",
            "  5 => 2009-2010\n",
            "  6 => 2011-2012\n",
            "  7 => 2013-2014\n",
            "  8 => 2015-2016\n",
            "  9 => 2017-2020\n",
            "Column 'demo_age_cat':\n",
            "  0 => 18 to 44\n",
            "  1 => 45 to 64\n",
            "  2 => 65 to 74\n",
            "  3 => 75+\n",
            "Column 'demo_race':\n",
            "  0 => Non-Hispanic White\n",
            "  1 => Hispanic\n",
            "  2 => Other\n",
            "  3 => Non-Hispanic Black\n",
            "  4 => Non-Hispanic Asian\n",
            "Column 'demo_race_black':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'demo_pregnant':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'demo_gender':\n",
            "  0 => Men\n",
            "  1 => Women\n",
            "Column 'bp_cat_meds_excluded':\n",
            "  0 => SBP 160+ or DBP 100+ mm Hg\n",
            "  1 => SBP of 130 to <140 or DBP 80 to <90 mm Hg\n",
            "  2 => SBP of 140 to <160 or DBP 90 to <100 mm Hg\n",
            "  3 => SBP <120 and DBP <80 mm Hg\n",
            "  4 => SBP of 120 to <130 and DBP <80 mm Hg\n",
            "Column 'bp_cat_meds_included':\n",
            "  0 => taking antihypertensive medications\n",
            "  1 => SBP of 130 to <140 or DBP 80 to <90 mm Hg\n",
            "  2 => SBP of 140 to <160 or DBP 90 to <100 mm Hg\n",
            "  3 => SBP 160+ or DBP 100+ mm Hg\n",
            "Column 'bp_control_jnc7':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_control_accaha':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_control_140_90':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_control_130_80':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_use':\n",
            "  0 => Yes\n",
            "  1 => No\n",
            "Column 'bp_med_recommended_jnc7':\n",
            "  0 => Yes\n",
            "  1 => No\n",
            "Column 'bp_med_recommended_accaha':\n",
            "  0 => Yes\n",
            "  1 => No\n",
            "Column 'bp_med_n_class':\n",
            "  0 => One\n",
            "  1 => None\n",
            "  2 => Two\n",
            "  3 => Three\n",
            "  4 => Four or more\n",
            "Column 'bp_med_n_pills':\n",
            "  0 => One\n",
            "  1 => None\n",
            "  2 => Two\n",
            "  3 => Three\n",
            "  4 => Four or more\n",
            "Column 'bp_med_combination':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_pills_gteq_2':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_ace':\n",
            "  0 => Yes\n",
            "  1 => No\n",
            "Column 'bp_med_aldo':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_alpha':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_angioten':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_beta':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_central':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_ccb':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_ccb_dh':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_ccb_ndh':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_diur_Ksparing':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_diur_loop':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_diur_thz':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_renin_inhibitors':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'bp_med_vasod':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'htn_jnc7':\n",
            "  0 => Yes\n",
            "  1 => No\n",
            "Column 'htn_accaha':\n",
            "  0 => Yes\n",
            "Column 'htn_aware':\n",
            "  0 => Yes\n",
            "  1 => No\n",
            "Column 'htn_resistant_jnc7':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'htn_resistant_accaha':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'htn_resistant_jnc7_thz':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'htn_resistant_accaha_thz':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'cc_smoke':\n",
            "  0 => Never\n",
            "  1 => Former\n",
            "  2 => Current\n",
            "Column 'cc_bmi':\n",
            "  0 => 30 to <35\n",
            "  1 => 35+\n",
            "  2 => 25 to <30\n",
            "  3 => <25\n",
            "Column 'cc_diabetes':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'cc_ckd':\n",
            "  0 => Yes\n",
            "  1 => No\n",
            "Column 'cc_cvd_mi':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'cc_cvd_chd':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'cc_cvd_ascvd':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'cc_cvd_hf':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'cc_cvd_any':\n",
            "  0 => No\n",
            "  1 => Yes\n",
            "Column 'DSDANCNT':\n",
            "  0 => 0\n",
            "  1 => 1\n",
            "  2 => 3\n",
            "  3 => 2\n",
            "  4 => 99\n",
            "  5 => 77\n",
            "Column 'DSD010AN':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 7\n",
            "  3 => 9\n",
            "Column 'DR1DRSTZ':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 5\n",
            "Column 'DR1DAY':\n",
            "  0 => 1\n",
            "  1 => 5\n",
            "  2 => 7\n",
            "  3 => 2\n",
            "  4 => 6\n",
            "  5 => 4\n",
            "  6 => 3\n",
            "Column 'DR1LANG':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 3\n",
            "  3 => 4\n",
            "  4 => 5\n",
            "  5 => 6\n",
            "Column 'DS1DS':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "Column 'DS1AN':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "Column 'DS1ANCNT':\n",
            "  0 => 0\n",
            "  1 => 1\n",
            "  2 => 2\n",
            "Column 'DR2DRSTZ':\n",
            "  0 => 1\n",
            "  1 => 5\n",
            "  2 => 2\n",
            "Column 'BPAARM':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 8\n",
            "Column 'BPACSZ':\n",
            "  0 => 4\n",
            "  1 => 3\n",
            "  2 => 5\n",
            "  3 => 2\n",
            "  4 => 1\n",
            "Column 'BPXPULS':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "Column 'BPXPTY':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "Column 'BPAEN1':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "Column 'BPAEN2':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "Column 'BPAEN3':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "Column 'LBXHA':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 3\n",
            "Column 'DBQ330':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 7\n",
            "  3 => 9\n",
            "Column 'BPQ020':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "Column 'BPQ030':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'BPQ040A':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "Column 'BPQ060':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'BPQ070':\n",
            "  0 => 3\n",
            "  1 => 9\n",
            "  2 => 4\n",
            "  3 => 1\n",
            "  4 => 2\n",
            "Column 'BPQ080':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'BPQ090D':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'CDQ010':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'HSQ590':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'DIQ010':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 3\n",
            "  3 => 9\n",
            "Column 'DIQ050':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'HIQ210':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'HUQ010':\n",
            "  0 => 4\n",
            "  1 => 3\n",
            "  2 => 1\n",
            "  3 => 2\n",
            "  4 => 5\n",
            "  5 => 9\n",
            "  6 => 7\n",
            "Column 'HUQ030':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 3\n",
            "  3 => 9\n",
            "  4 => 7\n",
            "Column 'HUQ090':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'HOQ065':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 3\n",
            "  3 => 7\n",
            "  4 => 9\n",
            "Column 'IMQ020':\n",
            "  0 => 1\n",
            "  1 => 3\n",
            "  2 => 9\n",
            "  3 => 2\n",
            "  4 => 7\n",
            "Column 'MCQ010':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'MCQ053':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'MCQ080':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 9\n",
            "Column 'MCQ092':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'MCQ160A':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'MCQ160B':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'MCQ160C':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'MCQ160D':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'MCQ160E':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'MCQ160F':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'MCQ160L':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'MCQ220':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'OSQ010A':\n",
            "  0 => 2\n",
            "  1 => 9\n",
            "  2 => 1\n",
            "Column 'OSQ010B':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'OSQ010C':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'OSQ060':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'PFQ059':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 7\n",
            "  3 => 9\n",
            "Column 'PFQ090':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'SMQ020':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'WHQ030':\n",
            "  0 => 1\n",
            "  1 => 2\n",
            "  2 => 3\n",
            "  3 => 9\n",
            "  4 => 7\n",
            "Column 'WHQ070':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'DPQ010':\n",
            "  0 => 1\n",
            "  1 => 0\n",
            "  2 => 2\n",
            "  3 => 3\n",
            "  4 => 9\n",
            "  5 => 7\n",
            "Column 'DPQ020':\n",
            "  0 => 3\n",
            "  1 => 1\n",
            "  2 => 0\n",
            "  3 => 2\n",
            "  4 => 9\n",
            "  5 => 7\n",
            "Column 'DPQ030':\n",
            "  0 => 3\n",
            "  1 => 1\n",
            "  2 => 0\n",
            "  3 => 2\n",
            "  4 => 7\n",
            "  5 => 9\n",
            "Column 'DPQ040':\n",
            "  0 => 3\n",
            "  1 => 1\n",
            "  2 => 0\n",
            "  3 => 2\n",
            "  4 => 9\n",
            "  5 => 7\n",
            "Column 'DPQ050':\n",
            "  0 => 2\n",
            "  1 => 0\n",
            "  2 => 3\n",
            "  3 => 1\n",
            "  4 => 9\n",
            "  5 => 7\n",
            "Column 'DPQ060':\n",
            "  0 => 1\n",
            "  1 => 0\n",
            "  2 => 3\n",
            "  3 => 2\n",
            "  4 => 9\n",
            "  5 => 7\n",
            "Column 'DPQ070':\n",
            "  0 => 0\n",
            "  1 => 3\n",
            "  2 => 1\n",
            "  3 => 2\n",
            "  4 => 7\n",
            "  5 => 9\n",
            "Column 'DPQ080':\n",
            "  0 => 0\n",
            "  1 => 1\n",
            "  2 => 2\n",
            "  3 => 3\n",
            "  4 => 9\n",
            "  5 => 7\n",
            "Column 'DPQ090':\n",
            "  0 => 0\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 2\n",
            "  4 => 3\n",
            "  5 => 7\n",
            "Column 'PUQ100':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "Column 'PUQ110':\n",
            "  0 => 2\n",
            "  1 => 9\n",
            "  2 => 1\n",
            "  3 => 7\n",
            "Column 'SLQ050':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "  2 => 9\n",
            "  3 => 7\n",
            "Column 'SMAQUEX':\n",
            "  0 => 2\n",
            "  1 => 1\n",
            "Column 'INDFMMPC':\n",
            "  0 => 3\n",
            "  1 => 1\n",
            "  2 => 2\n",
            "  3 => 7\n",
            "  4 => 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running TabNet for all datasets"
      ],
      "metadata": {
        "id": "B-zMnaQkGhn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for d_name in datasets.keys():\n",
        "    dataset_name = d_name\n",
        "    X_train, y_train, w_train, feat_list = splits[dataset_name]['train']\n",
        "    X_valid, y_valid, w_valid = splits[dataset_name]['valid']\n",
        "    X_test, y_test, w_test = splits[dataset_name]['test']\n",
        "\n",
        "    # pretraining\n",
        "    unsupervised_model = TabNetPretrainer() # define sampling weights here\n",
        "    max_epochs = 100\n",
        "    unsupervised_model.fit(\n",
        "        X_train=X_train,\n",
        "        eval_set=[X_valid],\n",
        "        max_epochs=max_epochs , patience=5,\n",
        "        batch_size=2048, virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        drop_last=False,\n",
        "        pretraining_ratio=0.5,\n",
        "    )\n",
        "\n",
        "    #unsupervised_model.save_model('/content/drive/MyDrive/Colab Notebooks/tabnet/test_pretrain')\n",
        "    #loaded_pretrain = TabNetPretrainer()\n",
        "    #loaded_pretrain.load_model('/content/drive/MyDrive/Colab Notebooks/tabnet/test_pretrain.zip')\n",
        "\n",
        "    clf = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
        "                          optimizer_params=dict(lr=2e-3),\n",
        "                          scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n",
        "                                            \"gamma\":0.9},\n",
        "                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "                          mask_type='sparsemax', # This will be overwritten if using pretrain model\n",
        "                          verbose=5,\n",
        "                          )\n",
        "    clf.fit(\n",
        "        X_train=X_train, y_train=y_train,\n",
        "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "        eval_name=['train', 'valid'],\n",
        "        eval_metric=['auc'],\n",
        "        max_epochs=max_epochs , patience=20,\n",
        "        batch_size=1024, virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        weights=1,\n",
        "        drop_last=False,\n",
        "        from_unsupervised=unsupervised_model,\n",
        "    )\n",
        "\n",
        "    preds = clf.predict(X_test)\n",
        "    f1 = f1_score(y_test, preds, sample_weight=w_test)\n",
        "\n",
        "    feature_importances = clf.feature_importances_\n",
        "    feature_indexes = np.arange(len(feature_importances))\n",
        "    indexed_importances = list(zip(feat_list, feature_importances))\n",
        "    sorted_indexed_importances = sorted(indexed_importances, key=lambda x: x[1], reverse=True)\n",
        "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
        "    sorted_importances = feature_importances[sorted_indices]\n",
        "    feature_indices = feat_list[sorted_indices]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))  # Adjust the figure size as necessary\n",
        "    plt.title(dataset_name)\n",
        "    plt.bar(feature_indices[:50], sorted_importances[:50], align='center')\n",
        "    plt.xticks(feature_indices[:50], rotation='vertical')  # Label x-ticks with feature indices\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xlabel('Features')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"/content/drive/MyDrive/Colab Notebooks/tabnet/results/{dataset_name}_feature_importances.png\")\n",
        "\n",
        "    # Create a DataFrame with feature names and importances\n",
        "    feature_data = pd.DataFrame({\n",
        "        'Feature': feature_indices,\n",
        "        'Importance': sorted_importances\n",
        "    })\n",
        "\n",
        "    # Add the validation and test scores as new rows\n",
        "    feature_data = feature_data.append([\n",
        "        {'f1-score': f1}\n",
        "    ], ignore_index=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    feature_data.to_csv(f'/content/drive/MyDrive/Colab Notebooks/tabnet/results/{dataset_name}_feature_importances_scores.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aJP3N9ieGh3O",
        "outputId": "4e54ecf1-6c94-454b-ba64-a5606bc7cb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-194-5965d5676627>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0munsupervised_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabNetPretrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# define sampling weights here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     unsupervised_model.fit(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/pretraining.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, eval_set, eval_name, loss_fn, pretraining_ratio, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, warm_start)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;31m# Apply predict epoch to all eval sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# Call method on_epoch_end for all callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/pretraining.py\u001b[0m in \u001b[0;36m_predict_epoch\u001b[0;34m(self, name, loader)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mlist_obfuscation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# Main loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobf_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mlist_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 3605 is out of bounds for axis 0 with size 2110"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}